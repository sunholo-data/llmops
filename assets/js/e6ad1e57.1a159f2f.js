"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[367],{3135:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var s=t(4848),r=t(8453);const i={},a="Parallel Execution",o={id:"howto/parallel",title:"Parallel Execution",description:"In many cases in GenAI its useful to call GenAI models or functions in parallel, to speed up user experience of the response.",source:"@site/docs/howto/parallel.md",sourceDirName:"howto",slug:"/howto/parallel",permalink:"/docs/howto/parallel",draft:!1,unlisted:!1,editUrl:"https://github.com/sunholo-data/sunholo-py/tree/main/docs/docs/howto/parallel.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Creating a grounded in Google Search VertexAI app",permalink:"/docs/howto/grounded_vertex"},next:{title:"Streaming",permalink:"/docs/howto/streaming"}},l={},u=[];function c(n){const e={code:"code",h1:"h1",p:"p",pre:"pre",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"parallel-execution",children:"Parallel Execution"}),"\n",(0,s.jsx)(e.p,{children:"In many cases in GenAI its useful to call GenAI models or functions in parallel, to speed up user experience of the response."}),"\n",(0,s.jsxs)(e.p,{children:["A basic ",(0,s.jsx)(e.code,{children:"asyncio"})," powered class is available via ",(0,s.jsx)(e.code,{children:"AsyncTaskRunner"})," to help facilitate this, primarily intended for API calls to VACs and agents."]}),"\n",(0,s.jsx)(e.p,{children:"It will wait for the first function to return and get the full result, before waiting for the next etc.   This is useful when constructing lots of context from different agents to feed into an orchestrator agent."}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import asyncio\nfrom sunholo.invoke import AsyncTaskRunner\nfrom sunholo.vertex import init_vertex, vertex_safety\nfrom vertexai.preview.generative_models import GenerativeModel\n\nasync def do_async(question):\n    # Initialize Vertex AI\n    init_vertex(location="europe-west1")\n    runner = AsyncTaskRunner(retry_enabled=True)\n\n    # Define async functions for runner\n    async def english(question):\n        print(f"This is English: question=\'{question}\'")\n        model = GenerativeModel(\n            model_name="gemini-1.5-flash",\n            safety_settings=vertex_safety(),\n            system_instruction="Answer in English"\n        )\n        result = await model.generate_content_async(question)\n        return result.text  # Assuming result has a \'text\' attribute\n\n    async def danish(question):\n        print(f"This is Danish: question=\'{question}\'")\n        model = GenerativeModel(\n            model_name="gemini-1.5-flash",\n            safety_settings=vertex_safety(),\n            system_instruction="Answer in Danish"\n        )\n        result = await model.generate_content_async(question)\n        return result.text\n\n    async def french(question):\n        print(f"This is French: question=\'{question}\'")\n        model = GenerativeModel(\n            model_name="gemini-1.5-flash",\n            safety_settings=vertex_safety(),\n            system_instruction="Answer in French"\n        )\n        result = await model.generate_content_async(question)\n        return result.text\n\n    async def italian(question):\n        print(f"This is Italian: question=\'{question}\'")\n        model = GenerativeModel(\n            model_name="gemini-1.5-flash",\n            safety_settings=vertex_safety(),\n            system_instruction="Answer in Italian"\n        )\n        result = await model.generate_content_async(question)\n        return result.text\n\n    # Add tasks to the runner\n    runner.add_task(english, question)\n    runner.add_task(french, question)\n    runner.add_task(danish, question)\n    runner.add_task(italian, question)\n\n    # Run tasks and process results as they complete\n    answers = {}\n    print(f"Start async run with {len(runner.tasks)} runners")\n    async for result_dict in runner.run_async_as_completed():\n        for func_name, result in result_dict.items():\n            if isinstance(result, Exception):\n                print(f"ERROR in {func_name}: {str(result)}")\n            else:\n                # Output the result\n                print(f"{func_name.capitalize()} answer:")\n                print(result)\n                answers[func_name] = result\n\n    # Return a dict of the results {"english": ..., "french": ..., "danish": ..., "italian": ...}\n    return answers\n\n# Run the asynchronous function\nif __name__ == "__main__":\n    question = "What is MLOps?"\n\n    # Run the do_async function using asyncio.run\n    answers = asyncio.run(do_async(question))\n\n    print("\\nFinal answers:")\n    for language, answer in answers.items():\n        print(f"{language.capitalize()}:\\n{answer}\\n")\n'})})]})}function d(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function a(n){const e=s.useContext(i);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),s.createElement(i.Provider,{value:e},n.children)}}}]);