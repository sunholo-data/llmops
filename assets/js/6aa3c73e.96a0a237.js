"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1222],{1786:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var t=i(4848),o=i(8453);const a={},r="LlamaIndex on VertexAI",s={id:"VACs/vertex-llamaindex",title:"LlamaIndex on VertexAI",description:"LlamaIndex is available within the VertexAI platform via a serverless integration - see here//cloud.google.com/vertex-ai/generative-ai/docs/model-reference/rag-api",source:"@site/docs/VACs/vertex-llamaindex.md",sourceDirName:"VACs",slug:"/VACs/vertex-llamaindex",permalink:"/docs/VACs/vertex-llamaindex",draft:!1,unlisted:!1,editUrl:"https://github.com/sunholo-data/sunholo-py/tree/main/docs/docs/VACs/vertex-llamaindex.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Pirate Talk",permalink:"/docs/VACs/pirate_talk"},next:{title:"Sunholo CLI",permalink:"/docs/cli"}},l={},d=[{value:"Setup",id:"setup",level:2},{value:"File Indexing",id:"file-indexing",level:2},{value:"Config",id:"config",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"llamaindex-on-vertexai",children:"LlamaIndex on VertexAI"}),"\n",(0,t.jsxs)(n.p,{children:["LlamaIndex is available within the VertexAI platform via a serverless integration - see here: ",(0,t.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/rag-api",children:"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/rag-api"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"sunholo"})," integrates with this application by providing HTTP endpoints for the indexing or new documents placed within Google Cloud Storage and via streaming or static VAC endpoints.  Whilst only some embedding features are implemented at the moment, the LlamaIndex on VertexAI integration takes care of a lot of aspects such as chunking and embedding, with no server to set up.  This makes it a good choice for quick and low-maintenance RAG applications."]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.p,{children:"You need a corpus ID created when you make one (only available via API at the moment):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import vertexai\nfrom vertexai.preview import rag\n\nvertexai.init(project=<project_id>, location="us-central1")\ncorpus = rag.create_corpus(display_name=..., description=...)\nprint(corpus)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Use the project_id, location and corpus_id within your config below."}),"\n",(0,t.jsx)(n.h2,{id:"file-indexing",children:"File Indexing"}),"\n",(0,t.jsxs)(n.p,{children:["Once your configuration is loaded within Multivac, embed and index them by adding files to your Google Cloud Storage bucket to have the files indexed, via ",(0,t.jsx)(n.a,{href:"../sunholo/llamaindex/import_files",children:(0,t.jsx)(n.code,{children:"llamaindex.import_files.py"})}),".  This supports large amounts of files."]}),"\n",(0,t.jsx)(n.h2,{id:"config",children:"Config"}),"\n",(0,t.jsx)(n.p,{children:"To use LlamaIndex on VertexAI, set up a memory store to send data to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"llm"}),' - LlamaIndex on VertexAI is only available on "vertex"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"model"})," - Needs to be one of the supported models listed ",(0,t.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/rag-api",children:"here"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"agent"})," - ",(0,t.jsx)(n.code,{children:"vertex-genai"})," is the VAC code shown in this example"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"display_name"})," - for UI integrations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"description"})," - for UI integrations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"memory"})," - configure the ",(0,t.jsx)(n.code,{children:"vectorstore"})," setting to ",(0,t.jsx)(n.code,{children:"llamaindex"})," to trigger sending data to the VertexAI rag corpus.  You can also send data to other memory types, such as ",(0,t.jsx)(n.code,{children:"alloydb"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"gcp_config"})," - settings that determine which VertexAI rag instance the data is sent to.  Only available in ",(0,t.jsx)(n.code,{children:"us-central1"})," at the moment.  ",(0,t.jsx)(n.code,{children:"rag_id"})," is the numeric identifier that you get when using ",(0,t.jsx)(n.code,{children:"rag.create()"})," to make the RAG corpus."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"chunker"})," - settings to configure on how LlamaIndex splits the data."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"kind: vacConfig\napiVersion: v1\nvac:\n  personal_llama:\n    llm: vertex\n    model: gemini-1.5-pro-preview-0514\n    agent: vertex-genai\n    display_name: LlamaIndex via Vertex AI\n    description: Use LlamaIndex on VertexAI via its vertex.rag integration\n    memory:\n      - llamaindex-native:\n          vectorstore: llamaindex\n    gcp_config:\n      project_id: llamaindex_project\n      location: us-central1 # only here at the moment\n      rag_id: 12341232323 # created via rag.create for now     \n    chunker:\n      chunk_size: 1000\n      overlap: 200\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);