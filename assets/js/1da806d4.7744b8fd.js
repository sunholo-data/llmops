"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4181],{8194:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var t=o(4848),r=o(8453);const a={},i="import_files.py",s={id:"sunholo/llamaindex/import_files",title:"import_files.py",description:"Source: sunholo/llamaindex/importfiles.py",source:"@site/docs/sunholo/llamaindex/import_files.md",sourceDirName:"sunholo/llamaindex",slug:"/sunholo/llamaindex/import_files",permalink:"/docs/sunholo/llamaindex/import_files",draft:!1,unlisted:!1,editUrl:"https://github.com/sunholo-data/sunholo-py/tree/main/docs/docs/sunholo/llamaindex/import_files.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"callback.py",permalink:"/docs/sunholo/langfuse/callback"},next:{title:"logging.py",permalink:"/docs/sunholo/logging"}},c={},l=[{value:"Functions",id:"functions",level:2},{value:"do_llamaindex(message_data, metadata, vector_name)",id:"do_llamaindexmessage_data-metadata-vector_name",level:3},{value:"get_corpus(gcp_config)",id:"get_corpusgcp_config",level:3},{value:"llamaindex_chunker_check(message_data, metadata, vector_name)",id:"llamaindex_chunker_checkmessage_data-metadata-vector_name",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"import_filespy",children:"import_files.py"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Source"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/sunholo-data/sunholo-py/blob/main/sunholo/llamaindex/import_files.py",children:"sunholo/llamaindex/import_files.py"})]}),"\n",(0,t.jsx)(n.h2,{id:"functions",children:"Functions"}),"\n",(0,t.jsx)(n.h3,{id:"do_llamaindexmessage_data-metadata-vector_name",children:"do_llamaindex(message_data, metadata, vector_name)"}),"\n",(0,t.jsx)(n.p,{children:"Configures and manages the corpus for a VertexAI project using the specified vector name\nby importing message data from Google Cloud Storage or Google Drive URLs."}),"\n",(0,t.jsx)(n.p,{children:"This function loads configuration from a YAML file, initializes a Vertex AI environment,\nand either fetches an existing corpus or creates a new one if it doesn't exist.\nIt supports importing files directly from cloud storage links."}),"\n",(0,t.jsx)(n.p,{children:"Parameters:\nmessage_data (str): The URL to the data on Google Cloud Storage or Google Drive that needs to be imported to the corpus.\nmetadata (dict): Additional metadata not explicitly used in this function but might be needed for extended functionality.\nvector_name (str): The name of the vector (and corpus) which will be used to locate and configure the specific settings from the configuration files."}),"\n",(0,t.jsx)(n.p,{children:"Raises:\nValueError: If the necessary configurations for GCP or project ID are not found, or if the corpus could not be established.\nNotImplementedError: If the data is not from supported sources (Google Cloud Storage or Google Drive)."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'message_data = "gs://bucket_name/path_to_file.txt"\nmetadata = {"user": "admin"}\nvector_name = "example_vector"\nresponse = do_llamaindex(message_data, metadata, vector_name)\nprint(response)\n# Imported file to corpus: {\'status\': \'success\'}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"get_corpusgcp_config",children:"get_corpus(gcp_config)"}),"\n",(0,t.jsx)(n.p,{children:"Retrieves a LlamaIndex corpus from Vertex AI based on the provided Google Cloud configuration."}),"\n",(0,t.jsx)(n.p,{children:"This function constructs a corpus name using project details from the configuration and attempts\nto fetch the corresponding corpus. If the corpus cannot be retrieved, it raises an error."}),"\n",(0,t.jsx)(n.p,{children:"Parameters:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["gcp_config (dict): Configuration dictionary that must include:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"project_id (str): Google Cloud project identifier."}),"\n",(0,t.jsx)(n.li,{children:"location (str): Google Cloud location."}),"\n",(0,t.jsx)(n.li,{children:"rag_id (str): Identifier for the RAG (Retrieval-Augmented Generation) corpus."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Returns:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The corpus object fetched from Vertex AI."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Raises:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ValueError: If any of the required configurations (project_id, location, or rag_id) are missing,\nor if the corpus cannot be retrieved."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example configuration dictionary\ngcp_config = {\n    'project_id': 'your-project-id',\n    'location': 'your-location',\n    'rag_id': 'your-rag-id'\n}\n\n# Fetch the corpus\ntry:\n    corpus = get_corpus(gcp_config)\n    print(\"Corpus fetched successfully:\", corpus)\nexcept ValueError as e:\n    print(\"Error fetching corpus:\", str(e))\n"})}),"\n",(0,t.jsx)(n.h3,{id:"llamaindex_chunker_checkmessage_data-metadata-vector_name",children:"llamaindex_chunker_check(message_data, metadata, vector_name)"}),"\n",(0,t.jsx)(n.p,{children:"No docstring available."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>s});var t=o(6540);const r={},a=t.createContext(r);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);