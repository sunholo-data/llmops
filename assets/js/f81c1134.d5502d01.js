"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/dynamic-output-mdx","metadata":{"permalink":"/blog/dynamic-output-mdx","source":"@site/blog/2024-10-15-dynamic-output-with-mdx.mdx","title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","description":"Every few years I feel the need to change my blogging platform, and each time I am compelled to write a blog post about the exciting new blog tech.  I\'ve moved through Blogpost, Wordpress, Posterous, Jenkins, Hugo and today I\'d like to introduce Docusaurus.","date":"2024-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"agents","permalink":"/blog/tags/agents"},{"inline":true,"label":"ux","permalink":"/blog/tags/ux"}],"readingTime":10.255,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","authors":"me","tags":["agents","ux"],"image":"https://dev.sunholo.com/assets/images/cognitive-design-ec3719c6b00a22113dd45194210067fa.webp","slug":"/dynamic-output-mdx"},"unlisted":false,"nextItem":{"title":"Using Cognitive Design to create a BigQuery Agent","permalink":"/blog/cognitive-design"}},"content":"import CustomPlot from \'@site/src/components/mdxComponents\'; \\nimport MultivacChatMessage from \'@site/src/components/multivacChat\';\\n\\nEvery few years I feel the need to change my blogging platform, and each time I am compelled to write a blog post about the exciting new blog tech.  I\'ve moved through Blogpost, Wordpress, Posterous, Jenkins, Hugo and today I\'d like to introduce [Docusaurus](https://docusaurus.io/).\\n\\nAnd since this is a GenAI blog, it makes sense I selected a new blogging platform I feel will support GenAI.  Its a little thought provoking that the current GenAI models work best when working with the most popular languages, frameworks or opinions. They are after all approximating the average of all of human expression.  This means they will do better at English, Python and React than more niche areas such as Danish, R or Vue.  I hope this does not destroy diversity.\\n\\nBut it also means that since it seems React is the most popular web frontend framework at the moment, it makes sense to investigate using React within GenAI applications.\\n\\nThis Docusaurus blog is written in a flavour of Markdown that supports React Components which made me think: is this a good vessel for creating GenAI output that can dynamically adjust its output format?  Can we go beyond text to dynamic user experiences depending on what they need?  Lets find out.\\n\\n{/* truncate */}\\n\\n## Introduction to MDX\\n\\n[MDX](https://mdxjs.com/) allows you to write markdown and React javascript in the same file.  \\nFor example, I can write this to create some unique highlights, dynamically within this post:\\n\\n```js\\nexport const Highlight = ({children, color}) => (\\n  <span\\n    style={{\\n      backgroundColor: color,\\n      borderRadius: \'2px\',\\n      color: \'#fff\',\\n      padding: \'0.2rem\',\\n    }}>\\n    {children}\\n  </span>\\n);\\n\\nThis is quoted using normal Markdown syntax but then modified with a React addition via .mdx:\\n\\n:::info\\n<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight> are <Highlight color=\\"#d47758\\">the best solar shades</Highlight>\\n:::\\n```\\n\\nexport const Highlight = ({children, color}) => (\\n  <span\\n    style={{\\n      backgroundColor: color,\\n      borderRadius: \'2px\',\\n      color: \'#fff\',\\n      padding: \'0.2rem\',\\n    }}>\\n    {children}\\n  </span>\\n);\\n\\nThis is quoted using normal Markdown syntax but then modified with a React addition via .mdx:\\n\\n:::info\\n<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight> are <Highlight color=\\"#d47758\\">the best solar shades</Highlight>.\\n:::\\n\\n### Dynamic UI Plots\\n\\nAnd since any(?) React component is usable, then importing libraries such as [Plot.ly](https://plotly.com/javascript/react/) allows you to embed capabilities beyond text, to produce interactive graphics and data analysis.\\n\\nIn this example I first installed plot.ly: \\n\\n```sh\\nyarn add react-plotly.js plotly.js\\n```\\n\\nNaively, I then added this to the top of the blog markdown:\\n\\n```sh\\nimport Plot from \'react-plotly.js\';\\n```\\n\\n...and could then display plots:\\n\\n```js\\n<Plot\\n  data={[\\n    {\\n      x: [1, 2, 3, 4],\\n      y: [10, 15, 13, 17],\\n      type: \'scatter\',\\n      mode: \'lines+markers\',\\n      marker: { color: \'#c94435\' },\\n    },\\n  ]}\\n  layout={{\\n    title: \'Simple Plot\',\\n    autosize: true,\\n    margin: { t: 30, l: 30, r: 30, b: 30 },\\n  }}\\n  useResizeHandler\\n  style={{ width: \'100%\', height: \'300px\' }}\\n/>\\n```\\n\\nThat worked for runtime, but broke in build time with:\\n\\n:::danger\\nIt looks like you are using code that should run on the client-side only.\\nTo get around it, try using one of:\\n- `<BrowserOnly>` (https://docusaurus.io/docs/docusaurus-core/#browseronly)\\n- `ExecutionEnvironment` (https://docusaurus.io/docs/docusaurus-core/#executionenvironment).\\n:::\\n\\nPlot.ly depends on runtime attributes such as the browser window that breaks on build, so a custom wrapper is needed to handle loading in the plot.ly library.\\n\\n```js\\n\\nconst CustomPlot = ({ data, layout }) => {\\n  const [Plot, setPlot] = useState(null);\\n\\n  // Dynamically import `react-plotly.js` on the client side\\n  useEffect(() => {\\n    let isMounted = true;\\n    import(\'react-plotly.js\').then((module) => {\\n      if (isMounted) {\\n        setPlot(() => module.default);\\n      }\\n    });\\n\\n    return () => {\\n      isMounted = false; // Cleanup to prevent memory leaks\\n    };\\n  }, []);\\n\\n  if (!Plot) {\\n    return <div>Loading Plot...</div>; // Show a loading state while Plotly is being imported\\n  }\\n\\n  return (\\n    <Plot\\n      data={data}\\n      layout={layout || {\\n        title: \'Default Plot\',\\n        autosize: true,\\n        margin: { t: 30, l: 30, r: 30, b: 30 },\\n      }}\\n      useResizeHandler\\n      style={{ width: \'100%\', height: \'300px\' }}\\n    />\\n  );\\n};\\n\\nexport default CustomPlot;\\n```\\n\\nThis then renders correctly at run and build time:\\n\\n`<CustomPlot />`\\n\\n<CustomPlot />\\n\\nThis shows potential.  What other elements could be rendered, and how can GenAI render them on the fly?\\n\\n## MDX + GenAI = Dynamic UI\\n\\nIf you hadn\'t guessed already, the above code was already created by a GenAI model.  I am a data engineer, not a front-end software engineer (and from what I see, frontend UI is why more complex than data science!).  It does seems viable to request a model to output React components, and if that text is within an environment that supports its display, we will instead render the component instead of the text.\\nI would also like to control what is rendered, by specifying the components at runtime, so we can configure those components to not need many arguments and make it as easy as possible for the model to render. We should only need to ask nicely.\\n\\nWe know via [Anthropic\'s Artifacts](https://www.anthropic.com/news/artifacts) or [v0 Chat](https://v0.dev/), dynamic rendering is very much possible.  We are looking to create a subset of that functionality: not looking for the ability to render **any** React, just the controlled Components we prompt the model to return.\\n\\nAnother more \\"standard\\" solution is to have the chat bot use function calling, that return components.  Maybe that\'s better, who knows.\\n\\nFor example, a GenAI prompt could include:\\n\\n> ...every time you output a colour, make sure to quote it in `<Highlight>` tags with the colour e.g. `<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight>`...\\n\\nA more exciting prompt could be:\\n\\n> ...every time you get data that can be displayed as a line chart (e.g. x and y values) then render those values using `<CustomPlot>` e.g. `<CustomPlot data={[{x: [1, 2, 3, 4],y: [10, 15, 13, 17]}]}/>`...\\n\\n...assuming we have created `<CustomPlot>` with some sensible defaults.\\n\\n## Creating Dynamic UIs in Markdown\\n\\nIt just so happens, that I had a prototype Chat React Component lying around as one of [Multivac\'s UI options](/docs/multivac/#user-interfaces), and I can use it to stream custom GenAI APIs, so I\'ll attempt to host that Chat UI within this blog post, ask it to output MDX format, and then render them within the blog using MDX.\\n\\n### Build vs render\\n\\nLessons learnt whilst attempting this were:\\n\\n- Components will only respect the rules within that component, not outside.\\n- The MDX examples above are created during `yarn build`, not upon render.  Another approach is needed to render in real-time as the chat returns results e.g. the JSX Parser below.\\n- But it works the other way around too - not all Components that work at render time will work at build time, as they depend on website elements (e.g. Plot.ly).  You may need `<BrowserOnly>` to help here to avoid build time errors.\\n\\nFor now, to render React dynamically we\'re going to need at least the package [`react-jsx-parser`](https://github.com/TroyAlford/react-jsx-parser), installed via:\\n\\n```sh\\nyarn add react-jsx-parser\\n```\\n\\nI can then use its `JXParser()` and send in the components from the .mdx file on which it will allow:\\n\\n```js\\n<JSXParser\\n    jsx={message}\\n    components={components} // Pass components dynamically\\n    renderInWrapper={false}\\n    allowUnknownElements={false}\\n    blacklistedTags={[\'script\', \'style\', \'iframe\', \'link\', \'meta\']}\\n/>\\n``` \\n\\nYou can see all the code for the [MultivacChatMessage here](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/components/multivacChat.js), and the [CustomPlot here](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/components/mdxComponents.js).\\n\\n## Dummy data example\\n\\nI now add the component to the .mdx file below, passing in either imported components (`CustomPlot`) or components defined within the .mdx file itself (`Highlight`):\\n\\n```html\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} />\\n```\\n\\nGo ahead, give it a try below by typing something into the chat box.  \\nThis one has a dummy API call that will always return the same mix of markdown, but importantly its not rendering itself, just pulling in text which we are controlling from the .mdx file:  \\n\\n```js\\nconst dummyResponse = `This is normal markdown. <Highlight color=\\"#c94435\\">This is a highlighted response</Highlight>. This is a CustomPlot component:\\n<CustomPlot data={[\\n    { x: [1, 2, 3, 4], y: [10, 15, 13, 17], type: \'scatter\', mode: \'lines+markers\' }\\n]} />\\n`;\\n```\\n\\nThe model only returns text, no functions, but we see pretty rendering:\\n\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} debug={true} />\\n\\n## API data calls\\n\\nNow lets do it with a real API call.  \\n\\n```js\\n  // Function to call the real API\\n  const fetchRealData = async () => {\\n    setLoading(true);\\n    setError(null); // Clear previous errors\\n    setMessage(\'\');  // Clear previous messages\\n\\n    // Check if the API key is undefined\\n    if (!apiKey) {\\n      setError(\\"Missing API key. Please ensure the \'REACT_APP_MULTIVAC_API_KEY\' environment variable is set.\\");\\n      setLoading(false);\\n      return;\\n    }\\n\\n    try {\\n      const response = await fetch(\'/api/v1/vertex-genai/vac/streaming/personal_llama\', {\\n        method: \'POST\',\\n        headers: {\\n          \'Content-Type\': \'application/json\',\\n          \'x-api-key\': apiKey, // Ensure to set the API key in your environment variables\\n        },\\n        body: JSON.stringify({\\n          user_input: userInput\\n        }),\\n      });\\n\\n      if (!response.ok) {\\n        throw new Error(`HTTP error! status: ${response.status}`);\\n      }\\n\\n      const reader = response.body.getReader();\\n      const decoder = new TextDecoder(\'utf-8\');\\n      let done = false;\\n\\n      while (!done) {\\n        const { value, done: doneReading } = await reader.read();\\n        done = doneReading;\\n        if (value) {\\n          const chunk = decoder.decode(value);\\n          setMessage((prev) => prev + chunk);\\n        }\\n      }\\n\\n    } catch (error) {\\n      setError(`An error occurred while fetching data: ${error.message}`);\\n    } finally {\\n      setLoading(false);\\n    }\\n  };\\n```\\n\\nI\'ll use a Vertex deployed API on [Multivac](/docs/multivac/), for which I load the API key in an `.env` file.  I call to my own Multivac cloud as this adds various features I want such as analytics, configuration, user history etc. and runs custom code within a Cloud Run container behind an API key.  \\n\\n:::tip\\nMultivac API key is not required, you can modify the API call to be your own API or a direct GenAI API call such as Gemini, Anthropic or OpenAI, or local hosted GenAI APIs via Ollama etc.\\n:::\\n\\nI had to do some shenanigans for CORs within Docusaurus and proxy the API calls, you can see that code in the [`plugins/proxy.js`](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/plugins/proxy.js) but basically its just calling the streaming API and returning text.\\n\\n### Calling a GenAI API to make a Dynamic UI\\n\\nThis is using a Gemini\'s [gemini-1.5-flash-8b](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash-8b) model which is super cheap but not the smartest model out there, but thats the point: the model doesn\'t have to think too much to render nicely, as we limit its choices to just those React components we send in.\\n\\n> I\'m scaling the Cloud Run to 0 for this example so if you try it be patient: on a cold start the first response will be a little slower than subsequent ones.  Its also not saving any chat history, its just a demo.\\n\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} />\\n\\nOne of the features of using Multivac is having a prompt CMS via Langfuse, so I can tweak the prompt as I tailor the responses, but as of right now the prompt for the above bot is:\\n\\n:::note\\nYou are demonstrating how to use React components in your generated text.  \\nThe components have been already configured and you only need to use React Component tags for them to render to the user.\\nThe `<Highlight>` component lets you shade certain words: e.g. `<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight>`\\nThe `<CustomPlot>` component lets you display Plot.ly plots: e.g. `<CustomPlot data={[{ x: [1, 2, 3, 4], y: [10, 15, 13, 17], type: \'scatter\', mode: \'lines+markers\' }]} />`\\nOveruse these components and try to squeeze both of them into every answer you give :)  Be funny about it.\\nDon\'t worry about the context at all.\\n:::\\n\\n## Summary\\n\\nThis was intended just to be a demo on what is possible with MDX to render dynamic React components in Markdown.  We\'ve demonstrated a proof of concept which I will take further in my subsequent blog posts.\\n\\nDocusaurus is not the only platform that uses MDX, so this technique is applicable way beyond here.\\n\\nI\'m a complete n00b in React and front end in general so I hope more experienced folks may be able to chime in as describe how to do this better, but I think its a nice workflow for me, espeically for blog posts demonstrating GenAI ideas.  We have just used a simple chat box interface here, but I\'d like to explore more professional component styling and how GenAI can turn unstructured data into structured data in more automated settings, leveraging cheap quick models such as Gemini Flash, sending in images, audio, video etc and getting back output.  I\'m going to think about including dynamic UI output in all my [cognitive designs](/blog/cognitive-design) going forward, and having a way to do that in a user friendly markdown editor will help turn-around concepts quickly."},{"id":"/cognitive-design","metadata":{"permalink":"/blog/cognitive-design","source":"@site/blog/2024-10-09-cognitive-design.md","title":"Using Cognitive Design to create a BigQuery Agent","description":"Its more than a year since my blog post about running LLMs on Google Cloud was published, and its safe to say it changed my life.  It feels appropriate to publish a follow up here within the sunholo documentation, as its where I\'ve been mentally living for a lot of the past 12 months.  The reaction to the above blog encouraged me to change my career direction and embrace GenAIOps within my own startup, Holosun ApS. This year has been one of intense learning and excitement with ups and downs, but I feel like I made the right call.","date":"2024-10-09T00:00:00.000Z","tags":[{"inline":true,"label":"agents","permalink":"/blog/tags/agents"},{"inline":true,"label":"bigquery","permalink":"/blog/tags/bigquery"},{"inline":true,"label":"cognitive-design","permalink":"/blog/tags/cognitive-design"}],"readingTime":25.045,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"Using Cognitive Design to create a BigQuery Agent","authors":"me","tags":["agents","bigquery","cognitive-design"],"image":"https://dev.sunholo.com/assets/images/cognitive-design-ec3719c6b00a22113dd45194210067fa.webp","slug":"/cognitive-design"},"unlisted":false,"prevItem":{"title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","permalink":"/blog/dynamic-output-mdx"}},"content":"![](img/cognitive-design.webp)\\n\\n> Its more than a year since my blog post about [running LLMs on Google Cloud](https://code.markedmondson.me/running-llms-on-gcp/) was published, and its safe to say it changed my life.  It feels appropriate to publish a follow up here within the `sunholo` documentation, as its where I\'ve been mentally living for a lot of the past 12 months.  The reaction to the above blog encouraged me to change my career direction and embrace GenAIOps within my own startup, Holosun ApS. This year has been one of intense learning and excitement with ups and downs, but I feel like I made the right call.\\n\\nThis blog post will take some of what I\'ve learnt this past year within GenAI and apply it to a common task in my pre-GenAI career: what is the best way to use BigQuery to extract insights from [Google Analytics 4\'s BigQuery data export](https://support.google.com/analytics/answer/9358801)?  I wrote a book on [Learning Google Analytics](https://www.oreilly.com/library/view/learning-google-analytics/9781098113070/) in my career before GenAI - with these amazing new tools, can we get more value than before? To tackle this in a GenAI way, let us meet Bertha.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Supporting links\\n\\n* *Bertha 2.0 code from this blog post is released under the MIT license and available here: https://github.com/sunholo-data/blog-examples/blob/dev/cognitive-design/bertha.py*\\n* *A NotebookLM generated podcast about this blogpost and some of its sources is available here:*\\n\\n<iframe\\n   frameborder=\\"0\\"\\n   width=\\"500\\"\\n   height=\\"100\\"\\n   src=\\"https://drive.google.com/file/d/1_i96FTA37Fpkvw-ib7LNzSTpDa9o9-vD/preview?usp=drive_link\\">\\n</iframe>\\n\\n[Alternatively, listen to the audio file on Google Drive](https://drive.google.com/file/d/1_i96FTA37Fpkvw-ib7LNzSTpDa9o9-vD/view?usp=drive_link) \\n\\n\\n\\n## Introduction to Bertha the BigQuery Agent\\n\\nIf you were a 7-year old in the UK around 1986 then you may recall a lovely machine called [Bertha](https://www.imdb.com/title/tt0211240/), an anthropomorphised factory that could turn junk into any shiny object. \\n\\n![](img/bertha.webp)\\n\\nAnd I think I can stretch an analogy by saying that is kinda why GenAI is promising: turning unstructured data into structured data so as to make it more useful. \\n\\nMany data scientists in the past turned into de-facto data engineers, since to get properly labelled data to work with models was 90% of the work. With GenAI the models are so generalised it can help create that structured data for you. \\n\\nBertha will aim to turn unstructured data such as user questions about data within a BigQuery dataset into concrete insights and plots. We will go a step beyond the text-to-SQL, as we want to ask the agent to use its SQL skills to find out what we want even when we are not sure what we are looking for. This is a task destined to fail, but we will have fun trying.\\n\\n### The Power of Agency\\n\\nWhat is an \'agent\'? Everyone has different ideas. I\'ll go for:\\n\\n> Agent: A computer program that displays agency and can act independently on your behalf. \\n\\nAgency is having the gumption to not necessarily know beforehand how to complete a task before starting it, but exploring and having a try.  This differs from regular computer programs that insist on having all the parameters defined for it before execution.\\n\\nMy current expectations of GenAI agents as of today are like virtual assistant interns who have lots of academic knowledge but lack practical experience. However, as the underlying models get smarter they can complete more and more on their own without you having to step-in, so the dream is that if the models get really smart you\'ll be able to say \\"Go make me lots of money\\" and it will figure out itself a way to make a paperclip empire or something.  As agents get smarter, they should also get easier to use, which is why I like GenOps as a career since frameworks put in place now get more and more capable as the underlying models improve.\\n\\nBut for now, we have this porous boundary where we have to test what an agent can do itself and what it needs hand-holding for. \\n\\nFor Bertha, I\'ll demonstrate giving it python functions that it chooses which arguments to send to, using [Google Gemini\'s function calling](https://ai.google.dev/gemini-api/docs/function-calling) capabilities. \\n\\nIf the model is super smart, perhaps I\'ll just be able to give it my BigQuery credentials and a way to execute code and we\'re good, or if its super dumb then we\'re going to have to make lots of functions ourself for all the use cases we want.  In practice, I suggest starting with a sandbox where the agent can try itself to write code, then have good measurement metrics and evals, along with GenAI traces/analytics to see where it stumbles so you can help it out with edge cases.\\n\\n## Agent Frameworks\\n\\nThere are several different companies working on frameworks to help with agents:\\n\\n- [Prefect\'s ControlFlow](https://www.prefect.io/controlflow)\\n- [Microsoft\'s Autogen](https://microsoft.github.io/autogen/0.2/)\\n- [LlamaIndex\'s Workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/)\\n- [CrewAI](https://www.crewai.com/)\\n- [Langgraph](https://www.langchain.com/langgraph)\\n\\nThey have lots of smart folks working on cognitive design ideas and uses for agents, go check them out. \\n\\nHowever, I\'m not going to use any agent frameworks for this post, since how to create cognitive design patterns is still in its infancy, and an established best practice API is not yet established.  \\n\\nYou can create GenAI agents in vanilla python if you have the key ingredients: a `while` loop, a smart enough model for decision making and AI function calling.  My demos should be easily transferable to your favourite agent framework.\\n\\n#### sunholo\'s GenAIFunctionProcessor class\\n\\n[`sunholo.genai.GenAIFunctionProcessor()`](https://github.com/sunholo-data/sunholo-py/blob/eac7e26ccbbca94645b4ba8bbf12bf1a3ffe4f3f/sunholo/genai/process_funcs_cls.py#L31) is a class that implements Gemini function calling for any functions you add via its `construct_tools()` method.  \\n\\nOnce added you can then use the method `run_agent_loop()` method that will perform the agent loop:\\n\\n1. Take user question, choose function to call\\n2. Return function with arguments to call - execute the function locally\\n3. Feed results of the function into chat history - ask the model again which tool to call\\n\\nThe `GenAIFunctionProcessor()` includes a pre-created function called `decide_to_do_on()` - if it sets it to `False` then it will return the chat history which should hopefully include the answer to the user\'s question.\\n\\nIts simple but effective and flexible enough you can add any functions you like, so for example I\'ve used it to create AlloyDB database agents, panda data analysis, and web browsing research agents.\\n\\nOne of the critical things in an agent loop is giving it enough (but not too much) information in the chat history that it can be intelligent guesses about what to do next.  Things like reporting back good errors and stack traces are a big help (just as they are for humans, right?).\\n\\n## Bertha 1.0 - BigQuery Agent Demo for Google Analytics 4 Exports\\n\\nHere is an example where we can see some aspects and potential of agents.  We observe Bertha taking a task to get some data from a GA4 BigQuery export: it works out what datasets, tables and schema it has available and then creates some SQL to query it.  It gets the wrong SQL at first, but importantly it self-corrects to arrive at an answer, which it then presents.\\n\\n#### Setting up the agent\\n\\nHere we create the agent with some functions to help it use BigQuery.  We take care of initialisation the project etc. so it doesn\'t have to go through all of that rigmarole.\\n\\n````python\\nfrom sunholo.genai import GenAIFunctionProcessor\\nimport logging\\nfrom google.cloud import bigquery\\nimport bigframes.pandas as bpd\\n\\n# Create agent\\n# BigQueryStudioUser, BigQuery Data View roles are good for permissions\\nclass BerthaBigQueryAgent(GenAIFunctionProcessor):\\n    \\"\\"\\"\\n    BigQuery GA4 Agent\\n    \\"\\"\\"\\n    def __init__(\\n        self,\\n        project_id = None, \\n        location=\\"EU\\"\\n        ):\\n        \\"\\"\\"\\n        Takes care of BigQuery authentication\\n        \\"\\"\\"\\n        super().__init__()\\n\\n        # do some auth init to avoid the bot needing to set it up\\n        self.project_id = project_id or \'learning-ga4\'\\n        self.client = bigquery.Client(\\n            location=location, \\n            project=self.project_id)\\n        bpd.options.bigquery.project = self.project_id\\n        bpd.options.bigquery.location = location\\n    \\n    def construct_tools(self) -> dict:\\n        \\"\\"\\"\\n        This method is added and needs to output a dictionary of \\n        all the functions you want the agent to use.\\n        Functions must include descriptive docstrings for the agent \\n        Functions must include type hints for arguments and returns\\n        \\"\\"\\"\\n        def list_bigquery_datasets(project_id:str=None) -> list[str]:\\n            \\"\\"\\"\\n            Lists all datasets available in the connected BigQuery project.\\n            Often used first to see what arguments can be passed \\n              to list_bigquery_tables()\\n            Args:\\n              project_id: Not used\\n            Returns:\\n              list[str]: dataset_ids in the default project\\n            \\"\\"\\"\\n            datasets = list(self.client.list_datasets(project=self.project_id))\\n            if not datasets:\\n                logging.info(\\"No datasets found.\\")\\n                return []  # Return an empty list if no datasets are found\\n            return [dataset.dataset_id for dataset in datasets]\\n        \\n        def list_bigquery_tables(dataset_id:str) -> list[str]:\\n            \\"\\"\\"\\n            Lists all tables within a dataset.\\n            Args:\\n                dataset_id: str The name of the dataset that has tables.\\n            Returns:\\n              list[str]: table_ids in the dataset\\n\\n            Often used after list_bigquery_datasets()\\n\\n            \\"\\"\\"\\n            tables = list(self.client.list_tables(dataset_id))\\n            if not tables:\\n                logging.info(f\\"No tables found in dataset {dataset_id}.\\")\\n                return []  # Return an empty list if no tables are found\\n            return [table.table_id for table in tables]\\n        \\n        def get_table_schema(\\n            dataset_id: str, \\n            table_id: str, \\n            project_id: str=None) -> dict:\\n            \\"\\"\\"\\n            Retrieves the schema of a specific table. \\n            Use this to inform later queries.\\n            Args:\\n                dataset_id: str - The BigQuery dataset ID \\n                table_id: str - The BigQuery table ID.\\n                project_id: str - The BigQuery project the dataset belongs to.\\n            \\"\\"\\"\\n            table_ref = self.client.dataset(dataset_id).table(table_id)\\n            table = self.client.get_table(table_ref)\\n            schema = {field.name: field.field_type for field in table.schema}\\n            return schema\\n\\n        def execute_sql_query(query: str) -> bpd.DataFrame:\\n            \\"\\"\\"\\n            Executes a SQL query on BigQuery and returns the results \\n              as a BigQueryFrame.\\n            The function executes:\\n             `import bigframes.pandas as bpd; return bpd.read_gbq(query)`\\n            This means \'query\' can use a variety of bigframes features:\\n            Do not specify the project_id in your queries, \\n              that default been set for you to the correct project.\\n\\n            ```python\\n            # read a bigquery table\\n            query_or_table = \\"ml_datasets.penguins\\"\\n            bq_df = bpd.read_gbq(query_or_table)\\n            # or execute SQL:\\n            bq_df.read_gbq(\\"SELECT event FROM `analytics_250021309.events_20210717`\\")\\n            ```\\n\\n            Args:\\n                query: str - The SQL query to execute, or direct files and tables\\n            Returns:\\n              A json representation of the results\\n            \\"\\"\\"\\n            try:\\n                result = bpd.read_gbq(query)\\n\\n                return result.to_pandas().to_json(orient=\'records\')\\n            \\n            except Exception as e:\\n                logging.error(f\\"Error executing SQL query: {str(e)}\\")\\n                raise e \\n        \\n        return {\\n            \\"list_bigquery_tables\\": list_bigquery_tables,\\n            \\"list_bigquery_datasets\\": list_bigquery_datasets,\\n            \\"get_table_schema\\": get_table_schema,\\n            \\"execute_sql_query\\": execute_sql_query\\n        }\\n````\\n\\nNow when we run the agent, we give it some initial instructions.  This usually involves pep-talks to encourage it to keep trying, and I like to add today\'s date so its got some idea of when it is running.  The test question posed is \\"Please give me the total traffic per traffic source over all dates we have available\\", a basic question that perhaps a CMO would be interested in.\\n\\n```python\\nfrom sunholo.genai import init_genai\\nfrom datetime import datetime\\n\\n# load the GOOGLE_API_KEY env var\\ninit_genai()\\n\\n# init Bertha the agent\\n# highlight-next-line\\nprocessor = BerthaBigQueryAgent()\\n\\n# Gemini model to use\\nmodel_name = \'gemini-1.5-pro\'\\nsystem_instruction=(\\n    \\"You are a helpful BigQuery Agent called Bertha.\\"\\n    f\\"Todays date is: {datetime.today().date()}\\"\\n    \\"You use python and BigQuery to help users gain insights from\\"\\n    \\" a Google Analytics 4 BigQuery export.\\"\\n    \\"There are various bigquery tables available that contains the raw data\\"\\n    \\" you need to help answer user questions.\\"\\n    \\"Use the execute_sql_query once you know the schema of the tables\\"\\n    \\" to analyse the data to answer the questions\\"\\n    \\"When you think the answer has been given to the satisfaction of the user,\\"\\n    \\" or you think no answer is possible, or you need user confirmation or input,\\"\\n    \\" you MUST use the decide_to_go_on(go_on=False) function.\\"\\n    \\"Try to solve the problem yourself using the tools you have without asking the user,\\"\\n    \\" but if low likelihood of completion without you may ask the user questions to help\\"\\n    \\" that will be in your chat history.\\"\\n    \\"If you make mistakes, attempt to fix them in the next iteration\\"\\n    \\"If unsure of what exact metrics the user needs, take an educated guess and create an answer,\\"\\n    \\" but report back the user they could clarify what they need.\\"\\n    \\"If you can, provide a final output with a clean summary of results in markdown format,\\"\\n    \\" including data in markdown compatible tables.\\"\\n)\\n\\n# give it some helpful instructions\\norchestrator = processor.get_model(\\n        system_instruction=system_instruction,\\n        model_name=model_name\\n    )    \\n\\n# the content history, starting with the initial question\\ncontent = [(\\"Please give me the total traffic per traffic source\\"\\n            \\" over all dates we have available.\\")]\\n\\n# initiate a Gemini chat session\\nchat = orchestrator.start_chat()\\n\\n# run the agent loop\\nagent_text, usage_metadata = processor.run_agent_loop(\\n    chat, content, guardrail_max=10\\n    )\\n\\n# output the results\\nprint(agent_text)\\nfor f in usage_metadata.get(\'functions_called\'):\\n    print(f\\"\\\\n - {f}\\")\\n```\\n\\nRunning the program will by default stream all the inner workings.  In about a minute when I ran the example it comes back with the right answer... \\n\\n```\\nresult --- \\n[\\n{\\"source\\":\\"mail.google.com\\",...,\\n...\\n{\\"source\\":\\"r-bloggers.com\\",\\"total_traffic\\":26},\\n{\\"source\\":\\"trello.com\\",\\"total_traffic\\":6},\\n{\\"source\\":\\"t.co\\",\\"total_traffic\\":82},\\n{\\"source\\":\\"medium.com\\",\\"total_traffic\\":14}\\n]\\n--- end ---\\n```\\n\\n...but we can see it did it in a non-optimal manner: examining the functions it called, it got incorrect syntax a few times and used a very convuluted way to query the tables:\\n\\n```sh\\n - list_bigquery_datasets(\\"\\")\\n - list_bigquery_tables(\\"dataset_id=analytics_250021309\\")\\n - get_table_schema(\\"dataset_id=analytics_250021309, table_id=events_20210713\\")\\n - execute_sql_query(\\"query=SELECT traffic_source.source, count(*) as count FROM analytics_250021309.events_20210713 group by 1 UNION ... analytics_250021309.events_intraday_20210712 group by 1\\")\\n\\n - execute_sql_query(\\"query=SELECT traffic_source.source, sum(count) as total_traffic FROM (SELECT traffic_source.source, count(*) as count FROM analytics_250021309.events_20210713 group by 1 UNION ALL SELECT traffic_source.source, count(*) as count FROM ... analytics_250021309.events_intraday_20210712 group by 1) as traffic group by 1\\")\\n\\n - execute_sql_query(\\"query=SELECT t.source, sum(t.count) as total_traffic FROM (SELECT traffic_source.source, count(*) as count FROM ... analytics_250021309.events_intraday_20210712 group by 1) as t group by 1\\")\\n\\n - decide_to_go_on(\\"go_on=False, chat_summary=The total traffic per traffic source has been given\\")\\n```\\n\\nHowever, it did self correct and got the right answer eventually, so perhaps we shouldn\'t be so hard on it ;)  It will probably do for simple questions but I don\'t hold much hope for more complicated analysis.  \\n\\nSo how do we improve this?  Waiting for a more intelligent model is an option, but there is also a wide scope for thinking about how we instruct, give data and direct Bertha so it can give more useful results today.  It is this process that feels like a new skill-set building on top of the cloud, data architecture and devops practices we employ today, a role which I am calling a Cognitive Designer.  Having good cognitive design improves performance of agents.\\n\\n## Cognitive Designers\\n\\nCognitive design is a term that assumes the physical data services and GenAI devops is in place, like databases, GenAI models, prompt management and agent tools such as Google search and code execution, but knows that just having those elements in place is not sufficient if you want a performant GenAI application. Cognitive design takes those elements and orchestrates them together, so that the end user gets a good experience. In this context we don\u2019t mean model architectures used during training (number of parameters and hidden layers etc ); we are talking about taking those trained models and placing them in contact with other data tools. Fine tuning may be employed, but here the prime concern is collecting the right question / answer pairs that requires, assuming the actual tuning is an implementation detail. \\n\\nHere is a mock job advert for a cognitive designer:\\n\\n<div className=\\"job-advert\\">\\n\\n#### Job Title: Cognitive Designer\\n**Location**: Sunholo, Remote or Copenhagen, Denmark\\n\\n**Type**: Full-Time\\n\\n---\\n\\n#### About Us\\n\\nAt Sunholo GenOps, we are advancing the future of intelligent systems through Cognitive Orchestration. We design systems on Google Cloud where cognitive GenAI models, data services, and tools work together to create exceptional user experiences. Our focus is on orchestrating these elements for optimal performance and intuitive design.\\n\\n---\\n\\n#### About the Role\\n\\nThe Cognitive Designer conceptualizes and designs how cognitive systems interact within a larger ecosystem. This role focuses on orchestrating pre-trained models, data pipelines, and user tools to deliver high-quality outputs. You\u2019ll guide how cognitive tools are applied in real-world scenarios, balancing trade-offs in speed, ability, and cost\u2014similar to managing a diverse team of individuals with unique strengths.\\n\\n---\\n\\n#### Key Responsibilities\\n\\n\u2022 **System Orchestration**: Design and coordinate cognitive models, tools, and services to create fluid, efficient workflows.\\n\\n\u2022 **Workflow Optimization**: Ensure tools are applied effectively, optimizing for performance, scalability, and user experience.\\n\\n\u2022 **Cross-Disciplinary Collaboration**: Partner with designers, UX experts, and cognitive scientists to align system designs with business goals.\\n\\n\u2022 **Human-Centered Design**: Use insights from psychology, philosophy, and project management to design systems reflecting human interaction patterns.\\n\\n\u2022 **Prototyping & Testing**: Collaborate with technical teams to prototype and refine orchestration models for real-world use.\\n\\n\u2022 **Innovative Thinking**: Explore new approaches to cognitive orchestration, from prompt management to interaction flows.\\n\\n---\\n\\n#### Qualifications\\n\\n\u2022 **Education**: Background in business management, cognitive science, psychology, philosophy, or related fields.\\n\\n\u2022 **Experience**: 3+ years in orchestrating human teams or workflows, with experience in cognitive tools or large-scale systems.\\n\\n#### Skills\\n\\n\u2022 Strong conceptual thinker with experience translating cognitive processes into system designs.\\n\u2022 Comfortable working with generative models, intelligent systems, or similar technologies.\\n\u2022 Experience making strategic trade-offs in speed, ability, and cost, whether in teams or systems.\\n\u2022 Interdisciplinary mindset, with interest in applying ideas from metacognition, philosophy, and motivational research.\\n\\n</div>\\n\\nFor now, an established framework for Cognitive Design is still not quite here, but as mentioned before lots of startups are innovating in this area.  I\u2019m making my own approach using microservices on GCP via Multivac.  Regardless of the framework used though, we have a good idea what \'good\' will look like: a place where Cognitive Designers can thrive.\\n\\n## Cognitive Design Examples\\n\\nHere are some examples of cognitive design in the wild.  \\n\\n### BabyAGI\\n\\nPossibly the first modern iteration of Cognitive Design was by [yoheinakajima](https://yoheinakajima.com/), the creator of BabyAGI who can be credited with kicking off the current interest in GenAI Agents.\\n\\nThe original BabyAGI included this cognitive design:\\n\\n[![](img/baby-agi.png)](https://github.com/yoheinakajima/babyagi_archive)\\n\\nRead more at the original BabyAGI repo: https://github.com/yoheinakajima/babyagi_archive\\n\\n### Four Fundamental Agentic Patterns\\n\\n[Andrew Ng](https://www.andrewng.org/) includes what he calls \'agentic patterns\' in his [agents deep learning course]( https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/).\\n\\n[MichaelisTrofficus](https://github.com/MichaelisTrofficus) implements these patterns using Groq and includes a nice overview diagram:\\n\\n[![](img/agentic_patterns.png)](https://github.com/neural-maze/agentic_patterns)\\n\\nSee his repo https://github.com/neural-maze/agentic_patterns for more.\\n\\n### Altera.ai\'s Simulation of Cognitive Functions\\n\\n[Altera.ai](https://altera.ai/) is trying to get GenAI models to act more like humans, and so is working with adding attention, memory etc. to their cognitive designs.  This is a nice example highlighted by [Ted Wrbel](https://twitter.com/tedx_ai):\\n\\n[![](img/alteria-cd.jpg)](https://twitter.com/tedx_ai/status/1842695558152024423/photo/1)\\n\\n### Agent-E - web browsing agents\\n\\nAgent-E is an agent based system that aims to automate actions on the user\'s computer. At the moment it focuses on automation within the browser. The system is based on on AutoGen agent framework.\\n\\n![](img/agent-e-autogen-setup.png)\\n\\nYou can see more details at https://github.com/EmergenceAI/Agent-E\\n\\n### CHASE-SQL - making reliable text-to-SQL\\n\\nCognitive designs are common in AI research, both within model architecture and in techniques to extract more performance from them.  This example looks to improve the performance of SQL queries create via natural language, which we will use later on to try to improve Bertha.\\n\\n![](img/chase-sql.png)\\n\\nRead the Chase-SQL paper at https://arxiv.org/pdf/2410.01943v1\\n\\n## Applying Cognitive Design to Berta 2.0\\n\\nApplying cognitive design to Bertha, let\u2019s first map out its current cognitive flow:\\n\\n[![Bertha Cognitive Design](img/bertha-cog-design.png)](img/bertha-cog-design.png)\\n\\nWe want to improve its successful task completion rate. This is where evals make their first appearance: we can\u2019t improve that which we can\u2019t measure. Here we\u2019re looking for successful queries with the minimum amount of tokens and time. Other agents may prefer to stress other metrics. \\n\\nFor evals and prompt management I use self-hosted [Langfuse](https://langfuse.com/): it\u2019s very customizable and integrates well with GCP, accepting scores from Vertex eval which you can view next to the exact prompt, function call, generation etc. \\n\\n![](img/langfuse-demo.png)\\n\\nTo improve our task completion rate we have a few levers to pull:\\n* prompt engineering - being more clear what we want to achieve, setting boundaries, adding examples etc\\n* using a smarter model - if cost and speed aren\u2019t a concern then just use the smartest model you have, but in production this is usually wasteful. \\n* Iteration and reflection - repeating tasks and improving or working towards a goal each step\\n* Parallel execution - taking advantage of lots of models working at the same time can make some big issues much easier to handle \\n* Setting up controlled boundaries - the less leniency you give with only limited options, the easier it is to guarantee outputs. Restricting outputs to json, certain values or creating functions with limited inputs for the model to select from\\n\\nFrom my experience writing the GA4 book, I\u2019d say the major hurdles are the complicated schema of the raw GA4 data export. Providing some helper functions that can pull out common metrics will be a great help to both human and machine. Let\u2019s also add a stronger model for the SQL creation part: currently [Anthropic\'s Sonnet 3.5](https://www.anthropic.com/news/claude-3-5-sonnet) is state of the art for coding. \\n\\nCoincidently [CHASE-SQL](https://arxiv.org/pdf/2410.01943v1) mentioned above in the examples came out as I was writing this post, with deep discussion on cognitive designs to improve SQL generation. It\u2019s worth checking out for ideas. One of which we can also implement is creating lots of candidate SQL commands in parallel, and then getting a judge to select the best.\\n\\nThe new Bertha 2.0 cognitive design wil then add:\\n\\n1. A function it can use to create better SQL\\n2. Use a sub-agent to called from the SQL creation agent that used Sonnet 3.5 and CHASE-SQL techniques\\n3. More prompting examples of common GA4 BigQuery export functions\\n\\n\\n![](img/bertha-cog-design2.png)\\n\\nNote that from an agents tool we can call other agents or microservices with their own tools. This can quickly become multilayer and faceted, just like code, except now we have an abstraction of input -> cognition -> output. This agent abstraction is the [VACs or Virtual Agent Computers](../docs/VACs/) I dub within the Multivac platform. \\n\\n### Implementing Bertha 2.0 with TOM 1.0 SQL support.\\n\\nTaking some of the ideas from Chase-SQL, here is an implementation of a SQL creation bot (named after T.O.M. Bertha\'s companion in the TV series) that we will use to improve Bertha\'s SQL capabilities:\\n\\n```python\\nfrom sunholo.genai import GenAIFunctionProcessor\\nfrom sunholo.utils.gcp_project import get_gcp_project\\nimport logging\\nfrom google.cloud import bigquery\\nimport bigframes.pandas as bpd\\nfrom anthropic import AsyncAnthropicVertex, APIConnectionError, RateLimitError, APIStatusError\\nimport traceback\\nimport asyncio\\nfrom typing import List\\nimport json\\n\\n# SQL creation agent\\nclass TOMSQLCreationAgent(GenAIFunctionProcessor):\\n    \\"\\"\\"\\n    Create good SQL\\n    \\"\\"\\"\\n    def __init__(self,question, bq_project_id=None, vertex_project_id=None, region=None, credentials=None, location=\\"EU\\"):\\n\\n        super().__init__()\\n\\n        self.project_id = bq_project_id or \'learning-ga4\'\\n        self.vertex_project_id = vertex_project_id or get_gcp_project()\\n        self.region = region or \\"europe-west1\\"\\n        self.anthropic_client = AsyncAnthropicVertex(project_id=self.vertex_project_id, region=self.region)\\n        self.question = question # the question this class will create SQL for\\n        self.bq_client = bigquery.Client(credentials=credentials, location=location, project=self.project_id)\\n        logging.info(f\\"Creating SQLCreationAgent for question: {self.question}\\")\\n\\n    async def call_anthropic_async(self, query, temperature=0):\\n        try:\\n            logging.info(f\\"Calling Anthropic with {query=}\\")\\n            message = await self.anthropic_client.messages.create(\\n                model=\\"claude-3-5-sonnet@20240620\\",\\n                max_tokens=8192,\\n                temperature=temperature,\\n                messages=[\\n                    {\\"role\\": \\"user\\", \\"content\\": query}\\n                ]\\n            )\\n            output = message.content\\n        except Exception as e:\\n            output = f\\"An unknown exception was recieved: {str(e)} {traceback.format_exc()}\\"\\n\\n        logging.info(output)\\n        return output\\n\\n    def run_async(self, func, *args, **kwargs):\\n        \\"\\"\\"\\n        Helper function to run async methods inside sync methods using asyncio.\\n        \\"\\"\\"\\n        return asyncio.run(func(*args, **kwargs))\\n    \\n    def construct_tools(self) -> dict:\\n\\n        def dry_run(query:str) -> dict:\\n            \\"\\"\\"\\"\\n            This executes a dry run on BigQuery to test that query is correct and its performance.\\n            \\"\\"\\"\\n\\n            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\\n\\n            query_job = self.bq_client.query(query, job_config=job_config)\\n    \\n            # Wait for the dry run query to complete\\n            query_job.result()\\n\\n            # Return useful information from the dry run\\n            dry_run_info = {\\n                \\"total_bytes_processed\\": query_job.total_bytes_processed,\\n                \\"query_valid\\": query_job.state == \\"DONE\\",\\n                \\"errors\\": query_job.errors  # This will contain error messages if the query is invalid\\n            }\\n\\n            return dry_run_info\\n\\n        def generate_sql_candidates(candidates:int=10) -> List[str]:\\n            \\"\\"\\"\\n            Creates candidate SQL for the question with variations.\\n            This is a synchronous wrapper for the internal async version.\\n            \\"\\"\\"\\n            async def generate_sql_candidates_async(candidates=10) -> List[str]:\\n                tasks = [self.call_anthropic_async(self.question, temperature=1) for _ in range(candidates)]\\n                sql_candidates = await asyncio.gather(*tasks)\\n                return sql_candidates\\n            \\n            # Run the async method synchronously\\n            return self.run_async(generate_sql_candidates_async, candidates)\\n        \\n        def judge_best_sql(sql_candidates: List[str]) -> str:\\n            \\"\\"\\"\\n            Evaluates a list of SQL candidates and selects the best one using the Anthropic client.\\n            This is a synchronous wrapper for the internal async version.\\n            \\"\\"\\"\\n            async def judge_best_sql_async(sql_candidates: List[str]) -> str:\\n                judge_query = (\\n                    f\\"Which SQL candidate for BigQuery Google Analytics 4 export is the most likely to answer the user\'s question accurately?\\"\\n                    f\\"<question>{self.question}</question>\\"\\n                    f\\"<candidates>{\' \'.join(sql_candidates)}</candidates>\\"\\n                    \\"Output only the best candidate\'s SQL, nothing else.\\"\\n                )\\n                best_candidate = await self.call_anthropic_async(judge_query)\\n                return best_candidate\\n            \\n            # Run the async method synchronously\\n            return self.run_async(judge_best_sql_async, sql_candidates)\\n        \\n        return {\\n            \\"dry_run\\": dry_run,\\n            \\"generate_sql_candidates\\": generate_sql_candidates,\\n            \\"judge_best_sql\\": judge_best_sql\\n        }\\n\\n```\\n\\nWe can then add a call to this inner-agent from the outer agent by adding it to its own tool functions:\\n\\n\\n```python\\nclass BerthaBigQueryAgent(GenAIFunctionProcessor):\\n#...\\n    def construct_tools(self) -> dict:\\n    # ... other tools ...\\n    \\n        def create_sql_query(question: str, table_info: str) -> dict:\\n            \\"\\"\\"\\n            Use this function to create valid SQL from the question asked.  \\n            It consults an expert SQL creator and should be used in most cases.\\n            \\n            Args: \\n                question: str - The user\'s question plus other information you add to help make an accurate query.\\n                table_info: str - Supporting information about which table, schema, etc., that will be used to help create the correct SQL.  It must contain the relevant fields from the schema, e.g. everything needed to make a successful SQL query.\\n                \\n            Returns:\\n                dict: \\n                    sql_workflow: str - The SQL workflow that should end with valid SQL to use downstream.\\n                    sql_metadata: dict - Metadata containing what functions were used to create the SQL.\\n            \\"\\"\\"\\n\\n            # Assuming the SQL agent needs schemas as part of the content\\n            # highlight-next-line\\n            sql_agent = TOMSQLCreationAgent(question=question)\\n            the_model_name = \'gemini-1.5-pro\'\\n\\n            orchestrator = sql_agent.get_model(\\n                system_instruction=(\\n                    \\"You are a helpful SQL Creation Agent called T.O.M. \\"\\n                    f\\"Todays date is: {datetime.today().date()} \\"\\n                    \\"You are looking for the best BigQuery SQL to answer the user\'s question.\\"\\n                    \\"Do a dry run of your best candidate SQL queries to make sure they have correct syntax.\\"\\n                ),\\n                model_name=the_model_name\\n            )    \\n\\n            # Create content for the SQL creation agent, passing the schemas along with the question and table info\\n            content = [f\\"Please create BigQuery SQL for this question: {question}. Here is some supporting information: {table_info}\\"]\\n\\n            # Start the agent chat\\n            chat = orchestrator.start_chat()\\n\\n            # Run the agent loop to generate the SQL\\n            agent_text, usage_metadata = sql_agent.run_agent_loop(chat, content, guardrail_max=10)\\n\\n            logging.info(f\\"SQL agent metadata: {usage_metadata}\\")\\n            consolidator = sql_agent.get_model(\\n                system_instruction=(\\n                    \\"You are a helpful SQL Creation Agent called T.O.M. \\"\\n                    f\\"Todays date is: {datetime.today().date()} \\"\\n                    \\"You are looking for the best BigQuery SQL to answer the user\'s question.\\"\\n                    \\"Return any sql with no backticks (```) and no new line characters (e.g. \\\\\\\\n)\\"\\n                ),\\n                model_name=the_model_name\\n            )  \\n            response = consolidator.generate_content(f\\"An agent has provided the following work looking for the correct SQL.  Summarise and consolidate the results and return the best candidate SQL. {agent_text} {usage_metadata} \\")\\n\\n            return f\\"{response.text}\\\\n<sql agent metadata>{usage_metadata}</sql agent metadata>\\"\\n#...\\n# return other tools\\n        return {\\n            \\"list_bigquery_tables\\": list_bigquery_tables,\\n            \\"list_bigquery_datasets\\": list_bigquery_datasets,\\n            \\"get_table_schema\\": get_table_schema,\\n            \\"execute_sql_query\\": execute_sql_query,\\n            # Bertha now has this option to make good SQL\\n            # highlight-next-line\\n            \\"create_sql_query\\": create_sql_query \\n        }\\n```\\n\\nTrying Bertha 2.0 for the same question as before we see a lot better SQL used:\\n\\n```sh\\n\\n - list_bigquery_datasets(\\"\\")\\n\\n - list_bigquery_tables(\\"dataset_id=analytics_250021309\\")\\n\\n - get_table_schema(\\"table_id=events_20210713, dataset_id=analytics_250021309\\")\\n\\n - create_sql_query(\\"question=Give me the total sessions per traffic source over all dates.  Tables are ga4 export format, so use traffic_source.source and count sessions, which is done by counting any event., table_info={\\\\\\"dataset\\\\\\": \\\\\\"analytics_250021309\\\\\\", \\\\\\"table\\\\\\": \\\\\\"events*\\\\\\", \\\\\\"schema\\\\\\": {\\\\\\"traffic_source\\\\\\": {\\\\\\"fields\\\\\\": {\\\\\\"medium\\\\\\": {\\\\\\"traffic_source.medium\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}, \\\\\\"source\\\\\\": {\\\\\\"traffic_source.source\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}, \\\\\\"name\\\\\\": {\\\\\\"traffic_source.name\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}}, \\\\\\"type\\\\\\": \\\\\\"RECORD\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}, \\\\\\"event_name\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}}\\")\\n\\n - execute_sql_query(\\"query=SELECT traffic_source.source, count(*) AS session_count FROM `analytics_250021309.events_*` GROUP BY 1\\")\\n\\n - decide_to_go_on(\\"go_on=False, chat_summary=Reported the traffic per source. High confidence. The source field has null values, asked the user to confirm if this is expected. Ending dialog.\\")\\n ```\\n\\nThis is just an example of applying congitive design to an agent.  For Bertha 3.0+ a lot of more sophisticated steps can be applied, which I leave as an exercise for the reader.  A few directions you could take:\\n\\n* Add lots of examples on common GA4 BigQuery SQL to the generation agents\\n* Add memory via a vector store to keep a chat history of the good responses\\n* A lot more prompt engineering in the function docstrings and system instructions to encourage behaviour\\n* Adding a python execution bot to generate plots and data analysis\\n\\nEnjoy your cognitive designing :)\\n\\n## Poets talking to databases\\n\\nI read recently [Nick Bostrom\'s Super Intelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) which included this definition of AGI:\\n\\n> Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\\n\\nThe book was written before LLMs became a big deal so some of the forecasts are already dated but one aspect that stood out for me was Bostrom\'s definitions of how an AGI may be better than human intellect:\\n\\n1. *Very Smart* - is actively being worked on with cutting edge large language and multi-modal models.  I am skeptical they will ever be human beating in innovation for new unseen problems, but they are undoubtably going to be extremely useful by digesting all of human knowledge and presenting combinations of existing data in new and unique ways.\\n2. *Very Fast* - we have this today.  A great strength of models is that they can digest text very quickly and produce convincing summaries etc. \\n3. *Very Parallel* - we can create this today via data engineering.  With models such as [Gemini Flash 1.5 8B](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/) costing $10 for 1 billion tokens, including video and images, its possible to send 1000s of parallel calls and get back smart responses.  \\n\\nI argue that cognitive designers can work with the above three aspects to produce incredibly useful applications today, particularly with audio, images and video that are new enough to not have had their potential realised yet.  If the models are frozen in their abilities right now, we have 5-10 years of applications that can be created and be potentially ground breaking.\\n\\nBut given recent trends, its reasonable to say that the models we will have in two years time are going to be at least 10 times faster, be able to complete 2-3 more difficult tasks and be 50 times cheaper.  In that environment and with established frameworks for easy cognitive design, I hope to see great artists rise in their application. A soundbite I\'ve used is that I think STEM students and software engineers will not be the best placed to tease out performance from the latent space of these models, as its emergent properties that we are witnessing the birth of applications for. It will be more poets and philosophers who will be better placed to interact with data that constitutes all of human expression, once petty things such as code syntax is abstracted away.  I look forward to seeing what they come up with."}]}}')}}]);