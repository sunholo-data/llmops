"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2309],{1582:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>x,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var a=r(4848),o=r(8453);const t={},i="Vertex AI",l={id:"integrations/vertexai",title:"Vertex AI",description:"Vertex AI Search",source:"@site/docs/integrations/vertexai.md",sourceDirName:"integrations",slug:"/integrations/vertexai",permalink:"/docs/integrations/vertexai",draft:!1,unlisted:!1,editUrl:"https://github.com/sunholo-data/sunholo-py/tree/main/docs/docs/integrations/vertexai.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ollama",permalink:"/docs/integrations/ollama"},next:{title:"Multivac Cloud",permalink:"/docs/multivac/"}},s={},c=[{value:"Vertex AI Search",id:"vertex-ai-search",level:2},{value:"Calling Vertex AI Search",id:"calling-vertex-ai-search",level:3},{value:"LlamaIndex on Vertex AI",id:"llamaindex-on-vertex-ai",level:2},{value:"Vertex Model Garden",id:"vertex-model-garden",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"vertex-ai",children:"Vertex AI"}),"\n",(0,a.jsx)(n.h2,{id:"vertex-ai-search",children:"Vertex AI Search"}),"\n",(0,a.jsx)(n.p,{children:"Formally called Enterprise Search and AI Search and Conversation, this is a data store chunk version."}),"\n",(0,a.jsxs)(n.p,{children:["Set ",(0,a.jsx)(n.code,{children:"vectorstore: vertex_ai_search"})," to use in your application"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"memory:\n    - discovery_engine_vertex_ai_search:\n        vectorstore: vertex_ai_search # or 'discovery_engine'\n"})}),"\n",(0,a.jsx)(n.h3,{id:"calling-vertex-ai-search",children:"Calling Vertex AI Search"}),"\n",(0,a.jsxs)(n.p,{children:["You can use ",(0,a.jsx)(n.code,{children:"vertex_ai_search"})," or ",(0,a.jsx)(n.code,{children:"llamaindex"})," specified below in your Vertex GenAI apps like this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from sunholo.utils.config import load_config_key\nfrom sunholo.vertex import init_vertex, get_vertex_memories, vertex_safety\n\nfrom vertexai.preview.generative_models import GenerativeModel, Tool\n\nvac_name = "must_match_your_vacConfig"\n\n# will init vertex client\ninit_vertex()\n\n# will look in your vacConfig for vertex-ai-search and llamaindex vectorstores\ncorpus_tools = get_vertex_memories(vac_name)\n\n# load model from config\nmodel = load_config_key("model", vac_name, kind="vacConfig")\n\n# use vertex Generative model with your tools\nrag_model = GenerativeModel(\n    model_name=model or "gemini-1.5-flash-001", \n    tools=corpus_tools,\n)\n\n# call the model\nresponse = rag_model.generate_content(contents, \n                                        safety_settings=vertex_safety(),\n                                        stream=True)\nfor chunk in response:\n    print(chunk)\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"The above assumes a vacConfig like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"kind: vacConfig\napiVersion: v1\nvac:\n  personal_llama:\n    llm: vertex\n    model: gemini-1.5-pro-preview-0514\n    agent: vertex-genai\n    display_name: Gemini with grounding via LlamaIndex and Vertex AI Search\n    memory:\n      - llamaindex-native:\n          vectorstore: llamaindex\n          rag_id: 4611686018427387904  # created via cli beforehand\n      - discovery_engine_vertex_ai_search:\n          vectorstore: vertex_ai_search # or discovery_engine\n"})}),"\n",(0,a.jsx)(n.h2,{id:"llamaindex-on-vertex-ai",children:"LlamaIndex on Vertex AI"}),"\n",(0,a.jsxs)(n.p,{children:["To use Llama Index on Vertex AI, set it as a ",(0,a.jsx)(n.code,{children:"memory"})," within your ",(0,a.jsx)(n.code,{children:"vacConfig"})," file."]}),"\n",(0,a.jsxs)(n.p,{children:["Set ",(0,a.jsx)(n.code,{children:"vectorstore: llamaindex"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"memory:\n    - llamaindex-native:\n        vectorstore: llamaindex\n        rag_id: 4611686018427387904 \n"})}),"\n",(0,a.jsx)(n.p,{children:"See above for code calling your data for RAG."}),"\n",(0,a.jsx)(n.h2,{id:"vertex-model-garden",children:"Vertex Model Garden"}),"\n",(0,a.jsxs)(n.p,{children:["To use GenAI model's deployed to Vertex Model Garden, you can set your 'llm' config and supply an ",(0,a.jsx)(n.code,{children:"endpoint_id"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"vac_model_garden:\n    llm: model_garden\n    gcp_config:\n        project_id: model_garden_project\n        endpoint_id: 12345678\n        location: europe-west1\n"})})]})}function x(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var a=r(6540);const o={},t=a.createContext(o);function i(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);