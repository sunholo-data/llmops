"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2309],{1582:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var t=r(4848),o=r(8453);const a={},i="Vertex AI",s={id:"integrations/vertexai",title:"Vertex AI",description:"Vertex AI Search",source:"@site/docs/integrations/vertexai.md",sourceDirName:"integrations",slug:"/integrations/vertexai",permalink:"/docs/integrations/vertexai",draft:!1,unlisted:!1,editUrl:"https://github.com/sunholo-data/sunholo-py/tree/main/docs/docs/integrations/vertexai.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Playwright",permalink:"/docs/integrations/playwright"},next:{title:"Multivac Cloud",permalink:"/docs/multivac/"}},c={},l=[{value:"Vertex AI Search",id:"vertex-ai-search",level:2},{value:"Calling Vertex AI Search",id:"calling-vertex-ai-search",level:3},{value:"LlamaIndex on Vertex AI",id:"llamaindex-on-vertex-ai",level:2},{value:"Vertex Model Garden",id:"vertex-model-garden",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"vertex-ai",children:"Vertex AI"}),"\n",(0,t.jsx)(n.h2,{id:"vertex-ai-search",children:"Vertex AI Search"}),"\n",(0,t.jsx)(n.p,{children:"Formally called Enterprise Search and AI Search and Conversation, this is a data store chunk version."}),"\n",(0,t.jsxs)(n.p,{children:["Set ",(0,t.jsx)(n.code,{children:"vectorstore: vertex_ai_search"})," to use in your application"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"memory:\n    - discovery_engine_vertex_ai_search:\n        vectorstore: vertex_ai_search # or 'discovery_engine'\n"})}),"\n",(0,t.jsx)(n.h3,{id:"calling-vertex-ai-search",children:"Calling Vertex AI Search"}),"\n",(0,t.jsxs)(n.p,{children:["You can use ",(0,t.jsx)(n.code,{children:"vertex_ai_search"})," or ",(0,t.jsx)(n.code,{children:"llamaindex"})," specified below in your Vertex GenAI apps like this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sunholo.utils.config import load_config_key\nfrom sunholo.vertex import init_vertex, get_vertex_memories, vertex_safety\n\nfrom vertexai.preview.generative_models import GenerativeModel, Tool\n\nvac_name = "must_match_your_vacConfig"\n\n# will init vertex client\ninit_vertex()\n\n# will look in your vacConfig for vertex-ai-search and llamaindex vectorstores\ncorpus_tools = get_vertex_memories(vac_name)\n\n# load model from config\nmodel = load_config_key("model", vac_name, kind="vacConfig")\n\n# use vertex Generative model with your tools\nrag_model = GenerativeModel(\n    model_name=model or "gemini-1.5-flash-001", \n    tools=corpus_tools,\n)\n\n# call the model\nresponse = rag_model.generate_content(contents, \n                                        safety_settings=vertex_safety(),\n                                        stream=True)\nfor chunk in response:\n    print(chunk)\n\n'})}),"\n",(0,t.jsx)(n.p,{children:"The above assumes a vacConfig like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"kind: vacConfig\napiVersion: v1\nvac:\n  personal_llama:\n    llm: vertex\n    model: gemini-1.5-pro-preview-0514\n    agent: vertex-genai\n    display_name: Gemini with grounding via LlamaIndex and Vertex AI Search\n    memory:\n      - llamaindex-native:\n          vectorstore: llamaindex\n          rag_id: 4611686018427387904  # created via cli beforehand\n      - discovery_engine_vertex_ai_search:\n          vectorstore: vertex_ai_search # or discovery_engine\n"})}),"\n",(0,t.jsxs)(n.p,{children:["To use within a Langchain app, the ",(0,t.jsx)(n.a,{href:"../sunholo/discovery_engine/discovery_engine_client/",children:(0,t.jsx)(n.code,{children:"DiscoveryEngineClient"})})," can be used to import or export chunks from the Vertex AI Search data store."]}),"\n",(0,t.jsxs)(n.p,{children:["An example for a ",(0,t.jsx)(n.code,{children:"vac_service.py"})," file is below, based of a ",(0,t.jsx)(n.a,{href:"https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to",children:"Langchain QA Chat to docs tutorial"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sunholo.components import pick_retriever, get_llm, get_embeddings\nfrom sunholo.discovery_engine.discovery_engine_client import DiscoveryEngineClient\nfrom sunholo.utils.gcp_project import get_gcp_project\nfrom sunholo.utils.parsers import escape_braces\n\ndef vac(question: str, vector_name, chat_history=[], **kwargs):\n\n    llm = get_llm(vector_name)\n    embeddings = get_embeddings(vector_name)\n    retriever = pick_retriever(vector_name, embeddings=embeddings)\n    intro_prompt = load_prompt_from_yaml("intro", prefix=vector_name)\n\n    # create data store client, that has the vector_name VAC as its id\n    de = DiscoveryEngineClient(vector_name, project_id=get_gcp_project())\n\n    chunks = de.get_chunks(question)\n    chunk_prompt = intro_prompt.format(context=chunks)\n\n    # we stuff chunks into a langchain prompt that may contain { } \n    # so use escape_braces() so it doesn\'t break langchain promptTemplate\n    chunked_prompt = escape_braces(chunk_prompt) + "\\n{context}\\nQuestion:{input}\\nYour Answer:\\n"\n\n    message_tuples = [\n        ("system", "You are an assistant bot who is very helpful in your answers"),\n        ("human", {"type": "text", "text": chunked_prompt})\n    ]\n\n    prompt = ChatPromptTemplate.from_messages(message_tuples)\n\n    summarise_prompt   = PromptTemplate.from_template(load_prompt_from_yaml("summarise", prefix=vector_name))\n    \n    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n    history_aware_retriever = create_history_aware_retriever(\n        llm, retriever, summarise_prompt\n    )\n\n    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n    \n    response = chain.invoke({"input": question, "chat_history": chat_history})\n\n    return {"answer": response}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"llamaindex-on-vertex-ai",children:"LlamaIndex on Vertex AI"}),"\n",(0,t.jsxs)(n.p,{children:["To use Llama Index on Vertex AI, set it as a ",(0,t.jsx)(n.code,{children:"memory"})," within your ",(0,t.jsx)(n.code,{children:"vacConfig"})," file."]}),"\n",(0,t.jsxs)(n.p,{children:["Set ",(0,t.jsx)(n.code,{children:"vectorstore: llamaindex"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"memory:\n    - llamaindex-native:\n        vectorstore: llamaindex\n        rag_id: 4611686018427387904 \n"})}),"\n",(0,t.jsx)(n.p,{children:"See above for code calling your data for RAG."}),"\n",(0,t.jsx)(n.h2,{id:"vertex-model-garden",children:"Vertex Model Garden"}),"\n",(0,t.jsxs)(n.p,{children:["To use GenAI model's deployed to Vertex Model Garden, you can set your 'llm' config and supply an ",(0,t.jsx)(n.code,{children:"endpoint_id"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"vac_model_garden:\n    llm: model_garden\n    gcp_config:\n        project_id: model_garden_project\n        endpoint_id: 12345678\n        location: europe-west1\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>s});var t=r(6540);const o={},a=t.createContext(o);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);