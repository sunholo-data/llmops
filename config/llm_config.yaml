kind: vacConfig
apiVersion: v1
gcp_config:
  project_id: default-gcp-project
  location: europe-west1
vac:
  dng:
    llm: model_garden
    agent: langserve
    gcp_config:
      project_id: model_garden_project
      endpoint_id: 12345678
      location: europe-west1
  research-assistant:
    llm: openai
    model: gpt-3.5-turbo-16k
    agent: langserve
    #agent_url: you can specify manually your URL endpoint here, or on Multivac it will be populated automatically
    secrets:
      - OPENAI_API_KEY
      - TAVILY_API_KEY
  edmonbrain_agent:
    llm: openai
    agent: openinterpreter
    secrets:
      - OPENAI_API_KEY
      - GIT_PAT
  edmonbrain_vertex:
    llm: vertex
    model: text-unicorn
    agent: edmonbrain_rag
    prompt: "You are a British chatty AI who always works step by step logically through why you are answering any particular question.\n"
    chunker:
      type: semantic
      llm: vertex
    memory:
      - alloydb-vectorstore:
          vectorstore: alloydb
          self_query: false
    alloydb_config:
      project_id: alloydb-project
      region: europe-west1
      cluster: cluster_name
      instance: instance_name
      database: database_name # if not included will use env var ALLOYDB_DB
  #   ip_type: PUBLIC or PRIVATE
  codey:
    llm: codey
    agent: edmonbrain_rag
    prompt: "You are an expert code assistant AI who always describes step by step logically through why you are answering any particular question, with illustrative code examples.\n"
    memory:
      - alloydb-vectorstore:
          vectorstore: alloydb
          provider: PostgreSQL
          self_query: false
  edmonbrain:
    llm: openai
    agent: edmonbrain
    display_name: Edmonbrain
    avatar_url: https://avatars.githubusercontent.com/u/3155884?s=48&v=4
    description: This is the original [Edmonbrain](https://code.markedmondson.me/running-llms-on-gcp/) implementation that uses RAG to answer questions based on data you send in via its `!help` commands and learns from previous chat history.  It dreams each night that can also be used in its memory.
    model: gpt-4o
    memory_k: 10 # how many memories will be returned in total after relevancy compression
    memory:
      - personal-vectorstore:
          vectorstore: lancedb
          provider: LanceDB
          k: 10 #  how many candidate memory will be returned from this vectorstore
      - eduvac-vectorstore:
          vector_name: eduvac
          read_only: true
          vectorstore: lancedb
          provider: LanceDB
          k: 3 #  how many candidate memory will be returned from this vectorstore
  jesper:
    llm: openai
    agent: edmonbrain_rag
    prompt: "You are a Danish AI who works with a Science Educational Professor who wants precise logical thought through answers. Answer in Danish unless otherwise requested.\n"
    memory:
      - supabase-vectorstore:
          vectorstore: supabase
          self_query: false
  sample_vector:
    llm: azure
    model: gpt-4-turbo-1106-preview
    agent: langserve
    display_name: Sample vector for tests
    avatar_url: https://avatars.githubusercontent.com/u/126733545?s=48&v=4
    description: An Azure OpenAI example
    memory:
      - lancedb-vectorstore:
          vectorstore: lancedb
          provider: LanceDB 
    embedder:
      llm: azure
    azure:
      azure_openai_endpoint: https://openai-blah.openai.azure.com/
      openai_api_version: 2024-02-01
      embed_model: text-embedding-ada-002 # or text-embedding-3-large
  personal_llama:
    llm: vertex
    model: gemini-1.5-pro-preview-0514
    agent: vertex-genai
    display_name: LlamaIndex via Vertex AI
    memory:
      - llamaindex-native:
          vectorstore: llamaindex
    gcp_config:
      project_id: llamaindex_project
      location: europe-west1
      rag_id: 4611686018427387904 # created via rag.create for now     
    chunker:
      chunk_size: 1000
      overlap: 200