kind: vacConfig
apiVersion: v1
gcp_config:
  project_id: multivac-internal-dev
  location: europe-west1
  endpoints_base_url: https://endpoints-xxxxx.a.run.app
vac:
  multivac_docs:
    llm: vertex
    model: gemini-1.0-pro
    agent: langserve
    display_name: Multivac
    tags: ["free"]
    avatar_url: https://avatars.githubusercontent.com/u/147247777?s=200&v=4
    description: What is Multivac? Talk to us about our Electric Dreams and hopes for the future. Explain to me below in the chat box what your business use case is and I will try to help you. If you don't have a use case right now, you can start with "What is Sunholo Multivac? or select another VAC from the drop down."
    memory:
      - lancedb-vectorstore:
          vectorstore: lancedb
    alloydb_config:
      project_id: multivac-alloydb
      region: europe-west1
      cluster: multivac-alloydb-cluster-20240401
      instance: primary-instance-1
  image_talk:
    llm: vertex
    model: gemini-1.0-pro-vision
    agent: langserve
    upload: 
      mime_types:
        - image
      buckets:
        image: multivac-internal-dev-dev-llmops-bucket
        all: multivac-internal-dev-dev-llmops-bucket
    display_name: Talk to Images
    tags: ["free"]
    avatar_url: https://avatars.githubusercontent.com/u/1342004?s=200&v=4
    description: I don't make pictures!  I read pictures you upload.  A picture is worth a thousand words, so upload your picture and ask your question to the Gemini Pro Vision model.  Images are remembered for your conversation until you upload another.  This offers powerful applications, which you can get a feel for via the [Gemini Pro Vision docs](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/design-multimodal-prompts) 
  pirate_speak:
    llm: openai
    agent: langserve
    display_name: Pirate Speak
    tags: ["free"]
    avatar_url: https://avatars.githubusercontent.com/u/126733545?s=48&v=4
    description: A Langchain demo using a demo [Langchain Template](https://templates.langchain.com/) that will repeat back what you say but in a pirate accent.  Ooh argh me hearties!  Langchain templates cover many different GenAI use cases and all can be streamed to Multivac clients.
    secrets:
      - OPENAI_API_KEY
  csv_agent:
    llm: openai
    agent: langserve
    display_name: Titanic
    tags: ["free"]
    avatar_url: https://avatars.githubusercontent.com/u/126733545?s=48&v=4
    description: Structured data such as in spreadsheets can provide challenging for traditional chat apps.  This Langchain app demonstrates how to use GenAI to generate data analysis python code, so that you can ask analyse a dataset with natural language. In this case, a local database contains statistics from the Titanic disaster passenger list.  Try asking "Did more people survive in first class than in other classes when the Titianic sank?".
    secrets:
      - OPENAI_API_KEY
  dng:
    llm: model_garden
    agent: langserve
    gcp_config:
      project_id: model_garden_project
      endpoint_id: 12345678
      location: europe-west1
  research-assistant:
    llm: openai
    model: gpt-3.5-turbo-16k
    agent: langserve
    #agent_url: you can specify manually your URL endpoint here, or on Multivac it will be populated automatically
    secrets:
      - OPENAI_API_KEY
      - TAVILY_API_KEY
  edmonbrain_agent:
    llm: openai
    agent: openinterpreter
    secrets:
      - OPENAI_API_KEY
      - GIT_PAT
  edmonbrain_vertex:
    llm: vertex
    model: text-unicorn
    agent: edmonbrain_rag
    prompt: "You are a British chatty AI who always works step by step logically through why you are answering any particular question.\n"
    chunker:
      type: semantic
      llm: vertex
    memory:
      - alloydb-vectorstore:
          vectorstore: alloydb
          self_query: false
    alloydb_config:
      project_id: alloydb-project
      region: europe-west1
      cluster: cluster_name
      instance: instance_name
      database: database_name # if not included will use env var ALLOYDB_DB
  #   ip_type: PUBLIC or PRIVATE
  codey:
    llm: codey
    agent: edmonbrain_rag
    prompt: "You are an expert code assistant AI who always describes step by step logically through why you are answering any particular question, with illustrative code examples.\n"
    memory:
      - alloydb-vectorstore:
          vectorstore: alloydb
          self_query: false
  edmonbrain:
    llm: openai
    agent: edmonbrain
    display_name: Edmonbrain
    avatar_url: https://avatars.githubusercontent.com/u/3155884?s=48&v=4
    description: This is the original [Edmonbrain](https://code.markedmondson.me/running-llms-on-gcp/) implementation that uses RAG to answer questions based on data you send in via its `!help` commands and learns from previous chat history.  It dreams each night that can also be used in its memory.
    model: gpt-4o
    memory_k: 10 # how many memories will be returned in total after relevancy compression
    memory:
      - discovery_engine_vertex_ai_search:
          vectorstore: vertex_ai_search # or discovery_engine
          
      - per_user_id:
          vectorstore: lancedb
          from_metadata_id: user_id # look up this value in the metadata - if present, put in vectorstore of that name
      - personal-vectorstore:
          vectorstore: lancedb
          k: 10 #  how many candidate memory will be returned from this vectorstore
      - eduvac-vectorstore:
          vector_name: eduvac
          read_only: true
          vectorstore: alloydb
          llm: openai
          k: 3 #  how many candidate memory will be returned from this vectorstore
  jesper:
    llm: openai
    agent: edmonbrain_rag
    prompt: "You are a Danish AI who works with a Science Educational Professor who wants precise logical thought through answers. Answer in Danish unless otherwise requested.\n"
    memory:
      - supabase-vectorstore:
          vectorstore: supabase
  sample_vector:
    llm: azure
    model: gpt-4-turbo-1106-preview
    agent: langserve
    display_name: Sample vector for tests
    avatar_url: https://avatars.githubusercontent.com/u/126733545?s=48&v=4
    description: An Azure OpenAI example
    memory:
      - lancedb-vectorstore:
          vectorstore: lancedb
          provider: LanceDB 
    embedder:
      llm: azure
    azure:
      azure_openai_endpoint: https://openai-blah.openai.azure.com/
      openai_api_version: 2024-02-01
      embed_model: text-embedding-ada-002 # or text-embedding-3-large
  personal_llama:
    llm: vertex
    model: gemini-1.5-pro-preview-0514
    agent: vertex-genai
    display_name: LlamaIndex via Vertex AI
    grounding:
      google_search: true
    memory:
      - llamaindex-native:
          vectorstore: llamaindex
          rag_id: 4611686018427387904 
      - agent_data_store:
          vectorstore: vertexai_agent_builder
          data_store_id: 1231231231231
    gcp_config:
      project_id: multivac-internal-dev
      location: us-central1  
    chunker:
      chunk_size: 1000
      overlap: 200